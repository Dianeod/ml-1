{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cross-Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cross-validation is a model validation technique that is used to assess how well the results produced by a model generalise to independent datasets. The aim of performing this technique is to train the algorithm using a variety of validation datasets in order to limit future problems with prediction, such as overfitting or underfitting.\n",
    "\n",
    "In this notebook, we demonstrate how to use the kdb+/q cross validation library on datasets in order to achieve accurate final results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading library scripts and data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following cell, the kdb+/q machine learning toolkit (ML-Toolkit) is loaded in to allow the use of functions provided in both the Utilities and Cross-Validation sections of the library. Graphics functions have also been loaded in for the purpose of this notebook.\n",
    "\n",
    "Data from the [\"Breast Cancer Wisconsin (Diagnostic) Data Set\"](https://www.kaggle.com/uciml/breast-cancer-wisconsin-data) is used below to predict whether a mass of cells is malignant or benign.\n",
    "\n",
    "The diagnosis column is removed from the dataset and used as the target vector as this is the feature we want to predict."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\\c 15 100\n",
    "\\l ../../ml.q\n",
    ".ml.loadfile`:init.q\n",
    "\\l graphics.q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The dataset contains 569 patient files.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "id          radius_mean texture_mean perimeter_mean area_mean smoothness_mean compactness_mean co..\n",
       "-------------------------------------------------------------------------------------------------..\n",
       "842302      17.99       10.38        122.8          1001      0.1184          0.2776           0...\n",
       "842517      20.57       17.77        132.9          1326      0.08474         0.07864          0...\n",
       "8.43009e+07 19.69       21.25        130            1203      0.1096          0.1599           0...\n",
       "8.43483e+07 11.42       20.38        77.58          386.1     0.1425          0.2839           0...\n",
       "8.43584e+07 20.29       14.34        135.1          1297      0.1003          0.1328           0...\n"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "`M`M`M`M`M`M`M`M`M`M`M`M`M`M`M`M`M`M`M`B`B`B`M`M`M`M`M`M`M`M`M`M`M`M`M`M`M`B`M`M`M`M`M`M`M`M`B`M`..\n"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data:(\"FS\",30#\"F\"; enlist \",\") 0:`:datasets/data.csv\n",
    "targets:select diagnosis from data\n",
    "data:delete diagnosis from data\n",
    "-1\"The dataset contains \",string[count data],\" patient files.\";\n",
    "5#data\n",
    "targets`diagnosis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One hot encoding is used on the target data to convert symbols into a numerical representation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 0 1 0..\n"
     ]
    }
   ],
   "source": [
    "show targets:exec diagnosis_M from .ml.onehot[targets;cols targets]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the cells below, polynomial features are produced from the original data table to allow for interactions between terms in the system. This allows us to study both individual and combinationed features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "radius_mean texture_mean perimeter_mean area_mean smoothness_mean compactness_mean concavity_mean..\n",
       "-------------------------------------------------------------------------------------------------..\n",
       "17.99       10.38        122.8          1001      0.1184          0.2776           0.3001        ..\n",
       "20.57       17.77        132.9          1326      0.08474         0.07864          0.0869        ..\n",
       "19.69       21.25        130            1203      0.1096          0.1599           0.1974        ..\n",
       "11.42       20.38        77.58          386.1     0.1425          0.2839           0.2414        ..\n",
       "20.29       14.34        135.1          1297      0.1003          0.1328           0.198         ..\n"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "/ target classifications should be agnostic of the id column\n",
    "5#table:(cols[data]except`id)#data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "radius_mean texture_mean perimeter_mean area_mean smoothness_mean compactness_mean concavity_mean..\n",
       "-------------------------------------------------------------------------------------------------..\n",
       "17.99       10.38        122.8          1001      0.1184          0.2776           0.3001        ..\n",
       "20.57       17.77        132.9          1326      0.08474         0.07864          0.0869        ..\n",
       "19.69       21.25        130            1203      0.1096          0.1599           0.1974        ..\n",
       "11.42       20.38        77.58          386.1     0.1425          0.2839           0.2414        ..\n",
       "20.29       14.34        135.1          1297      0.1003          0.1328           0.198         ..\n"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "/ add second order polynomial features to the table \n",
    "5#table:table^.ml.polytab[table;2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "radius_mean texture_mean perimeter_mean area_mean smoothness_mean compactness_mean concavity_mean..\n",
       "-------------------------------------------------------------------------------------------------..\n",
       "0.5210374   0.0226581    0.5459885      0.3637328 0.5937528       0.7920373        0.7031396     ..\n",
       "0.6431445   0.2725736    0.6157833      0.5015907 0.2898799       0.181768         0.2036082     ..\n",
       "0.6014956   0.3902604    0.5957432      0.4494168 0.5143089       0.4310165        0.4625117     ..\n",
       "0.2100904   0.3608387    0.2335015      0.1029056 0.8113208       0.8113613        0.5656045     ..\n",
       "0.6298926   0.1565776    0.6309861      0.4892895 0.4303512       0.3478928        0.4639175     ..\n"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "/ complete standard scaling of the dataset to avoid biases due to orders of magnitude in the data\n",
    "5#table:.ml.minmaxscaler[table]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "xtrain| +`radius_mean`texture_mean`perimeter_mean`area_mean`smoothness_mean`compactness_mean`conc..\n",
      "ytrain| 0 0 1 0 1 0 0 1 0 1 0 0 1 0 1 0 0 0 0 1 1 0 0 0 0 0 1 1 1 0 1 0 1 1 0 1 0 0 0 1 0 1 0 1 0..\n",
      "xtest | +`radius_mean`texture_mean`perimeter_mean`area_mean`smoothness_mean`compactness_mean`conc..\n",
      "ytest | 1 0 1 1 0 1 1 1 1 1 1 0 1 0 0 0 0 0 0 0 1 1 0 0 1 1 1 0 1 0 0 0 0 1 1 0 0 1 0 0 1 0 1 1 0..\n"
     ]
    }
   ],
   "source": [
    "/ complete a train-test-split on the data - below 20% of data is used in the test set\n",
    "show tts:.ml.traintestsplit[table;targets;.2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross-Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below a Random Forest Classifier model is initialized in order to classify tumours as malignant or benign. We can perform consistency checks on this model by performing cross validation techniques on the training data. In the first cell, cross-validation is applied in 5 folds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "k:5  / number of folds\n",
    "n:1  / number of repetitions\n",
    "\n",
    "xtrain:flip value flip tts`xtrain\n",
    "ytrain:tts`ytrain\n",
    "\n",
    "/ function with algorithm\n",
    "a:{.p.import[`sklearn.ensemble][`:RandomForestClassifier]}\n",
    "\n",
    "/ scoring function which takes a function, parameters to apply to that function and data as arguments\n",
    "score_func:.ml.xv.fitscore[a][`n_estimators pykw 500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/deanna/anaconda3/lib/python3.6/site-packages/sklearn/ensemble/weight_boosting.py:29: DeprecationWarning: numpy.core.umath_tests is an internal NumPy module and should not be imported. It will be removed in a future NumPy release.\n",
      "  from numpy.core.umath_tests import inner1d\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Model Scores:\n",
      "----------------------------------------------------------------------------\n",
      "Sequential split indices with basic k-fold cross validation: 0.9714286\n",
      "Random split indices with basic k-fold cross validation: 0.967033\n",
      "Stratified split indices with basic k-fold cross validation: 0.9758714\n"
     ]
    }
   ],
   "source": [
    "/ split data into k-folds and train/validate the model\n",
    "s1:.ml.xv.kfsplit[k;n;xtrain;ytrain;score_func]  / sequentially split\n",
    "s2:.ml.xv.kfshuff[k;n;xtrain;ytrain;score_func]  / randomized split\n",
    "s3:.ml.xv.kfstrat[k;n;xtrain;ytrain;score_func]  / stratified split\n",
    "\n",
    "-1\"Average Model Scores:\";\n",
    "-1\"----------------------------------------------------------------------------\";\n",
    "-1\"Sequential split indices with basic k-fold cross validation: \",string avg s1;\n",
    "-1\"Random split indices with basic k-fold cross validation: \",string avg s2;\n",
    "-1\"Stratified split indices with basic k-fold cross validation: \",string avg s3;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another option is to use repeated forms of cross validation, such as monte-carlo or repeated k-fold cross validation. These methods have the benefit of allowing a user to evaluate the consistency and robustness of the models produced. Below 5 folds are again used, this time with 5 repetitions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Model Scores:\n",
      "----------------------------------------------------------------------------\n",
      "Monte-Carlo cross validation with 5 repetitions and training size of 80%: 0.9626374\n",
      "Repeated stratified cross validation, 5 fold, 5 repetitions: 0.9727473\n",
      "Repeated sequential cross validation, 5 fold, 5 repetitions: 0.9731868\n"
     ]
    }
   ],
   "source": [
    "p:.2  / percentage of data in validation set\n",
    "n: 5  / number of repetitions\n",
    "\n",
    "r1:.ml.xv.mcsplit[p;n;xtrain;ytrain;score_func]\n",
    "r2:.ml.xv.kfshuff[k;n;xtrain;ytrain;score_func]\n",
    "r3:.ml.xv.kfsplit[k;n;xtrain;ytrain;score_func]\n",
    "\n",
    "-1\"Average Model Scores:\";\n",
    "-1\"----------------------------------------------------------------------------\";\n",
    "-1\"Monte-Carlo cross validation with 5 repetitions and training size of 80%: \",string avg r1;\n",
    "-1\"Repeated stratified cross validation, 5 fold, 5 repetitions: \",string avg r2;\n",
    "-1\"Repeated sequential cross validation, 5 fold, 5 repetitions: \",string avg r3;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grid Search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An alternative method is to perform a grid search over possible sets of hyperparameters in order to find the optimal model for the dataset. Grid search can be completed on the training data to find the best parameters which are then be applied to the model. Predictions are then made and scored using the unseen testing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "/ new scoring function\n",
    "sf:.ml.xv.fitscore[a]\n",
    "\n",
    "/ dictionary of parameters\n",
    "pd:`n_estimators`criterion`max_depth!(10 50 100 500;`gini`entropy;2 5 10 20 30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the grid search function below, the final argument is a float value denoting the size of the holdout set used in a fitted gridsearch where the best model is fit to holdout data. If 0 is used (shown below) the function will return scores for each fold for the given hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Grid search: hyperparameters and resulting score from each fold:\n",
      "\n",
      "n_estimators criterion max_depth|                                                                ..\n",
      "--------------------------------| ---------------------------------------------------------------..\n",
      "10           gini      2        | 0.978022  0.956044  0.9450549 0.9450549 0.956044  0.9450549 0.9..\n",
      "10           gini      5        | 0.9340659 0.978022  0.9340659 0.978022  0.978022  0.956044  0.9..\n",
      "10           gini      10       | 0.9450549 0.967033  0.967033  0.9340659 0.956044  0.956044  0.9..\n",
      "10           gini      20       | 0.9450549 0.967033  0.9450549 0.9450549 0.967033  0.978022  0.9..\n",
      "10           gini      30       | 0.9340659 0.978022  0.9340659 0.956044  0.9450549 0.9450549 0.9..\n",
      "10           entropy   2        | 0.967033  0.956044  0.989011  0.978022  0.967033  0.9340659 0.9..\n",
      "10           entropy   5        | 0.978022  0.967033  0.956044  0.989011  0.956044  0.967033  0.9..\n",
      "10           entropy   10       | 0.967033  0.967033  0.967033  0.967033  0.9340659 0.9450549 0.9..\n",
      "10           entropy   20       | 0.956044  0.967033  0.956044  0.967033  0.956044  0.9340659 0.9..\n",
      "10           entropy   30       | 0.978022  0.956044  0.967033  0.967033  0.956044  0.956044  0.9..\n",
      "50           gini      2        | 0.9340659 0.9450549 0.956044  0.9450549 0.967033  0.9450549 0.9..\n",
      "50           gini      5        | 0.9340659 0.978022  0.9340659 0.989011  0.978022  0.956044  0.9..\n",
      "..\n"
     ]
    }
   ],
   "source": [
    "-1\"Grid search: hyperparameters and resulting score from each fold:\\n\";\n",
    "show gr:.ml.gs.kfsplit[k;n;xtrain;ytrain;sf;pd;0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now fit this best model on our training set and test how well it generalises to new data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score for the 'best model' on the testing set was: 0.9736842\n"
     ]
    }
   ],
   "source": [
    "bstmdl:.p.import[`sklearn.ensemble][`:RandomForestClassifier][pykwargs first where a=max a:avg each gr]\n",
    "bstmdl[`:fit][xtrain;ytrain];\n",
    "br:bstmdl[`:score][flip value flip tts`xtest;tts`ytest]`\n",
    "-1\"Score for the 'best model' on the testing set was: \",string br;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternatively, the previous two cells can be compressed within a fitted grid search procedure. As explained above, this is done by using a float for the final parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "`n_estimators`criterion`max_depth!(500;`entropy;20)\n",
       "0.9912281\n"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "-2#.ml.gs.kfsplit[k;n;flip value flip table;targets;sf;pd;.2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The grid search function can also be passed a negative value for the final parameter. This means that data will be shuffled prior to designation of the holdout set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "`n_estimators`criterion`max_depth!(50;`entropy;20)\n",
       "0.9561404\n"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "-2#.ml.gs.kfsplit[k;n;flip value flip table;targets;sf;pd;-.2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cross validation is a useful technique to determine how well a model will generalize to new data. It is possible to carry out cross validation in a number of ways depending on the chosen dataset. Above we displayed how to perform cross validation using a range methods, which split data in a sequential, randomized or stratified manner, as well as using monte-carlo methods.\n",
    "\n",
    "It is clear that if the aim is to trial a range of hyperparameters on a model, then grid search is a robust way to test the chosen range of parameters and return the ones which allow the model to generalize best to new data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Q (kdb+)",
   "language": "q",
   "name": "qpk"
  },
  "language_info": {
   "file_extension": ".q",
   "mimetype": "text/x-q",
   "name": "q",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
